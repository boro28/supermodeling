{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# lab 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supermodeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## What is supermodeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Used Data Assimilation method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Submodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Lorenz again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![Lorenz equations](images/lorenz.png)\n",
    "\n",
    "source: https://en.wikipedia.org/wiki/Lorenz_system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import odeint\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import time\n",
    "from math import ceil\n",
    "from data_assimilation import assimilateLorenz\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeit(method):\n",
    "    def timed(*args, **kw):\n",
    "        ts = time.time()\n",
    "        result = method(*args, **kw)\n",
    "        te = time.time()\n",
    "        if 'log_time' in kw:\n",
    "            name = kw.get('log_name', method.__name__.upper())\n",
    "            kw['log_time'][name] = int((te - ts) * 1000)\n",
    "        else:\n",
    "            print('%r  %2.2f ms' % \\\n",
    "                  (method.__name__, (te - ts) * 1000))\n",
    "        return result\n",
    "    return timed\n",
    "\n",
    "\n",
    "def plot_models(models, timelines=None, figsize=(8,8)):\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    ax = fig.gca(projection=\"3d\")\n",
    "    for piece in models:\n",
    "            ax.plot(piece[:, 0].flatten(), piece[:, 1].flatten(), piece[:, 2].flatten(), lw=0.5)\n",
    "    plt.draw()\n",
    "    plt.show()\n",
    "    \n",
    "    if timelines:\n",
    "        plot_models_coords_in_time(models, timelines)\n",
    "    \n",
    "def plot_model(model, timeline=None, figsize=(5,5)):\n",
    "    plot_models([model], figsize=figsize)\n",
    "    if timeline is not None:\n",
    "        plot_model_coords_in_time(model, timeline)\n",
    "\n",
    "    \n",
    "def calc_mean_lorenz(lorenz_models):\n",
    "    return np.array([np.mean(lorenz_models[:,i,:], axis=0) for i in range(lorenz_models.shape[1])])\n",
    "\n",
    "\n",
    "def calc_weighted_average_lorenz(lorenz_models, weights):\n",
    "    return np.array([np.average(lorenz_models[:,i,:], axis=0, weights=weights) for i in range(lorenz_models.shape[1])])\n",
    "\n",
    "def lorenz_step(coords, parameters, dt=0.01):\n",
    "    rho, sigma, beta = parameters\n",
    "    x, y, z = coords\n",
    "    return coords + (np.array([sigma * (y - x), x * (rho - z) - y, x * y - beta * z]) * dt)\n",
    "\n",
    "def generate_lorenz(x=1, y=1, z=1, sigma=10.0, rho=28.0, beta=8.0/3.0, start_time=0.0, number_of_samples=4000, dt=0.01, plot=False):\n",
    "    size = number_of_samples\n",
    "    \n",
    "    if start_time:\n",
    "        size += ceil(start_time/dt)\n",
    "\n",
    "    coords = np.empty([size,3])\n",
    "\n",
    "    coords[0] = [x,y,z]\n",
    "    parameters = [rho, sigma, beta]\n",
    "    for i in range(size - 1):\n",
    "        coords[i+1] = lorenz_step(coords[i],parameters, dt=dt)\n",
    "    coords= coords[-number_of_samples:]\n",
    "    if plot:\n",
    "        plot_model(coords)\n",
    "    return coords, np.arange(start_time, start_time + number_of_samples * dt, dt)\n",
    "\n",
    "def plot_models_coords_in_time(models, timelines):\n",
    "    f, (px, py, pz) = plt.subplots(3, 1, sharex=True, figsize=(15,8))\n",
    "\n",
    "    px.set_ylabel(\"X component\")\n",
    "    py.set_ylabel(\"Y component\")\n",
    "    pz.set_ylabel(\"Z component\")\n",
    "    \n",
    "    for ix, (model, timeline) in enumerate(zip(models, timelines)):\n",
    "        px.plot(timeline, model[:,0], lw=0.5)\n",
    "        py.plot(timeline, model[:,1], lw=0.5)\n",
    "        pz.plot(timeline, model[:,2], lw=0.5, label=str(ix+1))\n",
    "\n",
    "    pz.set_xlabel(\"Time index (x 100)\")\n",
    "    \n",
    "    f.legend(title=\"Models in order\")\n",
    "    \n",
    "\n",
    "def plot_model_coords_in_time(model, timeline):\n",
    "    plot_models_coords_in_time([model], [timeline])\n",
    "    \n",
    "def lorenz_research(\n",
    "    start_time = 4.5,\n",
    "    number_of_samples = 200,\n",
    "    steps_4dvar = 20,\n",
    "    small_plot = True,\n",
    "    full_plot = True,\n",
    "    initial_params = None\n",
    "):\n",
    "    l63_gt, l63_timeline_gt = generate_lorenz(\n",
    "        start_time=start_time, \n",
    "        number_of_samples=number_of_samples\n",
    "    )\n",
    "    \n",
    "    \n",
    "    initial = dict(\n",
    "        x=l63_gt[0][0], \n",
    "        y=l63_gt[0][1],\n",
    "        z=l63_gt[0][2],\n",
    "        sigma = 10.0,\n",
    "        rho = 20.,\n",
    "        beta = 5.\n",
    "    )\n",
    "    \n",
    "    if initial_params is not None:\n",
    "        initial.update(initial_params)\n",
    "    \n",
    "    error_vector = [0.1, 0.1, 0.1, 100, 100, 100]\n",
    "    \n",
    "    coords_and_params = assimilateLorenz(\n",
    "        initial, \n",
    "        l63_gt,\n",
    "        lorenz_step, \n",
    "        maximum_number_of_steps=steps_4dvar,\n",
    "        verbose=True, \n",
    "        error_vector=error_vector\n",
    "    )\n",
    "    l63_4dv, l63_4dv_timeline = generate_lorenz(\n",
    "        x=l63_gt[0][0], \n",
    "        y=l63_gt[0][1],\n",
    "        z=l63_gt[0][2],\n",
    "        sigma=coords_and_params['sigma'],\n",
    "        rho=coords_and_params['rho'],\n",
    "        beta=coords_and_params['beta'],\n",
    "        number_of_samples=number_of_samples\n",
    "    )\n",
    "    \n",
    "    print(l63_gt, l63_timeline_gt)\n",
    "    \n",
    "    if small_plot:\n",
    "        plot_models(\n",
    "            [l63_gt, l63_4dv],\n",
    "            [l63_timeline_gt, l63_timeline_gt]\n",
    "        )\n",
    "    \n",
    "    \n",
    "    full_lorenz, fl_timeline = generate_lorenz()\n",
    "    full_predicted, full_pred_timeline = generate_lorenz(sigma=coords_and_params['sigma'],rho=coords_and_params['rho'],beta=coords_and_params['beta'])\n",
    "        \n",
    "    if full_plot:\n",
    "        plot_models([full_lorenz, full_predicted], [fl_timeline, full_pred_timeline])\n",
    "        \n",
    "    return {\n",
    "        'partial': {\n",
    "            'gt': [l63_gt, l63_timeline_gt],\n",
    "            'pred': [l63_4dv, l63_4dv_timeline],\n",
    "        },\n",
    "        'full': {\n",
    "            'gt': [full_lorenz, fl_timeline],\n",
    "            'pred': [full_predicted, full_pred_timeline],\n",
    "        }\n",
    "    }\n",
    "\n",
    "def set_submodel_consts(\n",
    "    number_of_samples = 200,\n",
    "    steps_4dvar = 20\n",
    "):    \n",
    "    def create_submodel(sigma, rho, beta, start_time = 4.5):\n",
    "            \n",
    "        return lorenz_research(    \n",
    "            start_time = start_time,\n",
    "            number_of_samples = number_of_samples,\n",
    "            steps_4dvar = steps_4dvar,\n",
    "            small_plot = True,\n",
    "            full_plot = False,\n",
    "            initial_params = dict(sigma=sigma,rho=rho,beta=beta)\n",
    "        )\n",
    "\n",
    "    return create_submodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Choose a fragment of lorenz 63 attractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first task you have to sample a chosen part of Lorenz attractor. \\\n",
    "Remember that the attractor uses time so in other word `models_start` variable \\\n",
    "is a time from which we start sampling with `dt = 0.01` so plots are smoother.\n",
    "`number_of_samples = 200` is maximum value for next tasks so check lower "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: find fancy, not too long trajectory of L63\n",
    "model_start = ???\n",
    "number_of_samples = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l63, l63_timeline = generate_lorenz(\n",
    "    start_time=model_start, \n",
    "    number_of_samples=number_of_samples\n",
    ")\n",
    "plot_model(l63)\n",
    "plot_model_coords_in_time(l63, l63_timeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Have fun with 4dvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So you know how to sample L63 trajectory. \\\n",
    "Now you will use 4dvar to predict lorenz64 parameters.\n",
    "Find out how many samples are needed so we can say that the prediction is ok. \\\n",
    "`number_of_samples` should be less than `200`, otherwise the prediction would have taken too much time.\n",
    "Try few options each one in new cell so you can compare your results. \\\n",
    "TIP: play on range [5, 200]. \\\n",
    "Put `;` on the end of the method, it disables auto printing of returned value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lorenz_research(\n",
    "    number_of_samples=???\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Play with iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4dvar assimilation does iterations and during each one it gets closer \\\n",
    "to the solution. Play with them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lorenz_research(\n",
    "    number_of_samples=???,\n",
    "    steps_4dvar=???\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4. Create submodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use the idea that some pretrained models can be connected to create new, \\\n",
    "better one. In this task you have to create 3 submodels with chosen by you \\\n",
    "init values. We hope you have some intuition after previous tasks so you can \\\n",
    "chose them wisely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submodel_creator = set_submodel_consts(\n",
    "    number_of_samples = ???,\n",
    "    steps_4dvar = ???\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm1 = submodel_creator(\n",
    "    sigma=???,\n",
    "    rho=???,\n",
    "    beta=???,\n",
    "    start_time=???\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm2 = submodel_creator(\n",
    "    sigma=???,\n",
    "    rho=???,\n",
    "    beta=???,\n",
    "    start_time=???\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm3 = submodel_creator(\n",
    "    sigma=???,\n",
    "    rho=???,\n",
    "    beta=???,\n",
    "    start_time=???\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_models([\n",
    "    sm1['full']['pred'][0],\n",
    "    sm2['full']['pred'][0],\n",
    "    sm3['full']['pred'][0],\n",
    "],\n",
    "[\n",
    "    sm1['full']['pred'][1],\n",
    "    sm2['full']['pred'][1],\n",
    "    sm3['full']['pred'][1],\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_submodel = calc_mean_lorenz(np.array([\n",
    "    sm1['full']['pred'][0],\n",
    "    sm2['full']['pred'][0],\n",
    "    sm3['full']['pred'][0],\n",
    "]))\n",
    "\n",
    "plot_models([sm1['full']['gt'][0], mean_submodel], [sm1['full']['gt'][1], sm1['full']['gt'][1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5. Mean average from submodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some times one of your model can be better than others so its importance \\\n",
    "might be higher than others. Try to find best weights for weight average of your submodels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [?,?,?]\n",
    "\n",
    "mean_submodel = calc_weighted_average_lorenz(np.array([\n",
    "        sm1['full']['pred'][0],\n",
    "        sm2['full']['pred'][0],\n",
    "        sm3['full']['pred'][0],\n",
    "    ]),\n",
    "    weights\n",
    ")\n",
    "\n",
    "plot_models([sm1['full']['gt'][0], mean_submodel], [sm1['full']['gt'][1], sm1['full']['gt'][1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6. 3d-var for weighted average from submodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try to use 3d-var to find best weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obertvator_kind(kind):\n",
    "    def observator(weights):\n",
    "        return calc_weighted_average_lorenz(np.array([\n",
    "                sm1[kind]['pred'][0],\n",
    "                sm2[kind]['pred'][0],\n",
    "                sm3[kind]['pred'][0],\n",
    "            ]),\n",
    "            list(np.ravel(weights))\n",
    "        )\n",
    "    return observator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_weights_full = assimilate_lorenz_weighted([1.,1.,1.], sm1['full']['gt'][0], obertvator_kind('full'))\n",
    "predicted_weights_partial = assimilate_lorenz_weighted([1.,1.,1.], sm1['partial']['gt'][0], obertvator_kind('partial'))\n",
    "\n",
    "weighted_avg_on_full = calc_weighted_average_lorenz(np.array([\n",
    "            sm1['full']['pred'][0],\n",
    "            sm2['full']['pred'][0],\n",
    "            sm3['full']['pred'][0],\n",
    "        ]),\n",
    "    predicted_weights_full\n",
    "    )\n",
    "\n",
    "weighted_avg_on_partial = calc_weighted_average_lorenz(np.array([\n",
    "            sm1['full']['pred'][0],\n",
    "            sm2['full']['pred'][0],\n",
    "            sm3['full']['pred'][0],\n",
    "        ]),\n",
    "    predicted_weights_partial\n",
    "    )\n",
    "\n",
    "plot_models(\n",
    "    [weighted_mean,\n",
    "     weighted_avg_on_full, \n",
    "     weighted_avg_on_partial], \n",
    "    [sm1['full']['gt'][1],\n",
    "     sm1['full']['gt'][1],\n",
    "     sm1['full']['gt'][1]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_models(\n",
    "    [sm1['full']['gt'][0],\n",
    "    ??? # put here your best weighted avg supermodelą\n",
    "    ], \n",
    "    [sm1['full']['gt'][1],\n",
    "     sm1['full']['gt'][1]]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "4dvar_seir_new.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
